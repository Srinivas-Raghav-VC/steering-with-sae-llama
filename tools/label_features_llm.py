#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Post-hoc LLM feature labeling utility.

Reads he_pipeline_results/results_sae_only.json, gathers a few top-activating
texts per selected feature, calls Gemini to assign short labels, and writes
he_pipeline_results/feature_labels.json. You can then re-run the pipeline with:

  python he_steer_pipeline.py --use-feature-labels --feature-labels-path he_pipeline_results/feature_labels.json

Requirements:
- GEMINI_API_KEY in env (for google-generativeai)
- Existing results file generated by he_steer_pipeline.py

Notes:
- If top-activating texts are not present, this script can optionally re-load
  the model and mine top texts using the same helper in the pipeline. This is
  slower and optional (enabled with --mine-top-texts).
"""
import argparse
import json
import os
from pathlib import Path
from typing import Dict, List
import torch

# Use vendored Gemini labeler (with graceful fallback)
GeminiFeatureLabeler = None  # type: ignore
try:
    # Try relative import first
    try:
        from .gemini_feature_labeler import GeminiFeatureLabeler  # type: ignore
    except ImportError:
        # Try absolute import from tools
        import sys
        sys.path.append(os.path.dirname(os.path.abspath(__file__)))
        from gemini_feature_labeler import GeminiFeatureLabeler  # type: ignore
except Exception:
    GeminiFeatureLabeler = None  # type: ignore

# Optional: import pipeline bits to mine top texts
try:
    from he_steer_pipeline import (
        Config,
        setup_environment,
        load_model_and_tokenizer,
        JumpReLUSAE,
        top_activating_texts_for_feature,
        load_text_pairs_splits,
    )
except Exception:
    Config = None  # type: ignore
    setup_environment = None  # type: ignore
    load_model_and_tokenizer = None  # type: ignore
    JumpReLUSAE = None  # type: ignore
    top_activating_texts_for_feature = None  # type: ignore
    load_text_pairs_splits = None  # type: ignore


def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--results", type=str, default="he_pipeline_results/results_sae_only.json")
    ap.add_argument("--out", type=str, default="he_pipeline_results/feature_labels.json")
    ap.add_argument("--limit", type=int, default=200, help="max features to label")
    ap.add_argument("--mine-top-texts", action="store_true", help="mine top texts by running model if needed")
    ap.add_argument("--per-feature-texts", type=int, default=3)
    args = ap.parse_args()

    results_path = Path(args.results)
    if not results_path.exists():
        raise SystemExit(f"Results file not found: {results_path}")

    with open(results_path, "r", encoding="utf-8") as f:
        res = json.load(f)

    selected_layers = res.get("selected_layers") or []
    top_hi = res.get("top_features_hi") or {}
    top_en = res.get("top_features_en") or {}

    # Build feature prompt bundles: for each feature, attach a few texts
    feature_prompts: Dict[str, Dict] = {}

    # Optionally prepare model and texts for mining
    model = tok = None
    cfg = None
    va_hi = va_en = None
    if args.mine_top_texts and Config is not None:
        cfg = Config()
        setup_environment()
        model, tok = load_model_and_tokenizer(cfg)
        _, _, va_hi, va_en, _, _ = load_text_pairs_splits(cfg)

    def ensure_texts(layer: int, feat_idx: int, side: str) -> List[str]:
        # Try to mine top activating texts if we have the model and data
        if model is not None and tok is not None and top_activating_texts_for_feature is not None:
            d = model.config.hidden_size
            sae_path = Path(cfg.ckpt_dir) / f"sae_layer{layer}_best.pth"
            if sae_path.exists():
                sae = JumpReLUSAE(d, cfg.sae_expansion_factor, cfg.sae_l0_target).to(cfg.device)
                state = torch.load(sae_path, map_location=cfg.device)
                sae.load_state_dict(state, strict=False)
                sae.eval()
                texts_src = va_hi if side == "hi" else va_en
                try:
                    return top_activating_texts_for_feature(model, tok, cfg, sae, layer, texts_src, feat_idx, sample_n=600)[: args.per_feature_texts]
                except Exception:
                    return []
        return []

    # Accumulate features
    count = 0
    for L in selected_layers:
        Ls = str(L)
        for side, feats in [("hi", top_hi.get(Ls, [])), ("en", top_en.get(Ls, []))]:
            for fi in feats:
                key = f"L{L}:{fi}"
                if key in feature_prompts:
                    continue
                feature_prompts[key] = {
                    "layer": L,
                    "feature": fi,
                    "side": side,
                    "examples": [],
                }
                count += 1
                if count >= args.limit:
                    break
            if count >= args.limit:
                break
        if count >= args.limit:
            break

    # Attach example texts
    for key, meta in feature_prompts.items():
        examples = meta.get("examples", [])
        # We don't have cached top-texts in results, so optionally mine
        if not examples and args.mine_top_texts:
            ex = ensure_texts(meta["layer"], meta["feature"], meta["side"]) or []
            meta["examples"] = ex

    if GeminiFeatureLabeler is None:
        raise SystemExit("GeminiFeatureLabeler not available. Ensure tools/gemini_feature_labeler.py exists.")

    # Proceed even without API key; the vendored labeler will fall back to a heuristic
    labeler = GeminiFeatureLabeler(api_key=os.getenv("GEMINI_API_KEY"))
    used_gemini = bool(getattr(labeler, "_client", None))
    if not used_gemini:
        print("⚠️  Gemini unavailable – falling back to heuristic feature labels.")

    labels: Dict[str, str] = {}
    for key, meta in feature_prompts.items():
        try:
            label = labeler.label_feature(feature_id=key, examples=meta.get("examples", []))
        except Exception as e:
            label = f"{key}: Language-selective feature"
        labels[key] = label

    out_path = Path(args.out)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    payload = {
        "labels": labels,
        "metadata": {
            "total_features": len(labels),
            "used_gemini": used_gemini,
        },
    }
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2, ensure_ascii=False)
    mode = "Gemini" if used_gemini else "heuristic"
    print(f"Wrote {len(labels)} labels to {out_path} ({mode})")


if __name__ == "__main__":
    main()
